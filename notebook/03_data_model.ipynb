{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mZillowHouseData\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_training\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataModeling\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mZillowHouseData\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mZillowHouseData\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomException\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from src.ZillowHouseData.components.model_training import DataModeling\n",
    "from src.ZillowHouseData.logger import logger\n",
    "from src.ZillowHouseData.exception import CustomException\n",
    "from src.ZillowHouseData.utils.common import save_model_to_keras, save_object_to_pickle, read_csv_to_dataframe, prepare_data, train_and_test_split\n",
    "from src.ZillowHouseData.utils import common\n",
    "from src.ZillowHouseData.config.configuration import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE_NAME = \"Data Modelling\"\n",
    "class DataModellingPipeline:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def data_model(self):\n",
    "        # Create an instance of the Modelling class   \n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} initiated <<<<<<\\n\\nx==========x\")\n",
    "        file_path = os.path.join('artifacts', 'data_ingestion', 'final.csv')\n",
    "\n",
    "        # Read data\n",
    "        df = read_csv_to_dataframe(file_path)\n",
    "        logger.info(\">>>>>> CSV read <<<<<<\\n\\nx==========x\")\n",
    "\n",
    "        logger.info(f\">>>>>> Preparing data fror training <<<<<<\\n\\nx==========x\")\n",
    "        # Preprocess data\n",
    "        X, y, label_to_category_mapping = prepare_data(df)\n",
    "        save_object_to_pickle(label_to_category_mapping, \"models\", \"label\")\n",
    "\n",
    "        #Train test Split\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test, scaler = train_and_test_split(X, y)\n",
    "        logger.info(\">>>>>> Test train split completed <<<<<<\\n\\nx==========x\")\n",
    "\n",
    "        save_object_to_pickle( X_test_scaled, \"models\", \"X_test_scaled.pkl\")\n",
    "        save_object_to_pickle( y_test, \"models\", \"y_test.pkl\")\n",
    "        save_object_to_pickle(scaler, \"models\", \"scaler.pkl\")\n",
    "        logger.info(\">>>>>> Saved X_test and y_test as pickle for model evaluation <<<<<<\\n\\nx==========x\")\n",
    "\n",
    "\n",
    "        config = ConfigurationManager()\n",
    "        model_training_config = config.get_model_training_config()\n",
    "        data_modeling = DataModeling(config = model_training_config)\n",
    "\n",
    "        # Build model\n",
    "        model = data_modeling.build_model(X_train_scaled.shape[1])\n",
    "        logger.info(\">>>>>> Model building completed<<<<<<\\n\\nx==========x\")\n",
    "\n",
    "        logger.info(\">>>>>> Train model <<<<<<\\n\\nx==========x\")\n",
    "        model = data_modeling.train_model(model, X_train_scaled, y_train)\n",
    "        logger.info(\">>>>>> Model training completed<<<<<<\\n\\nx==========x\")\n",
    "\n",
    "        logger.info(\">>>>>> Model Summary <<<<<<\\n\\nx==========x\\n\")\n",
    "        model_details = model.summary()\n",
    "        logger.info(\">>>>>> Model Summary <<<<<<\\n\\nx==========x\\n\\n\")\n",
    "        logger.info(model_details)\n",
    "\n",
    "        save_model_to_keras(model, \"models\", \"model.keras\")\n",
    "        logger.info(\">>>>>> Saved model as pickle <<<<<<\\n\\nx==========x\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "        obj2 = DataModellingPipeline()\n",
    "        obj2.data_model()\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "from src.ZillowHouseData.exception import CustomException\n",
    "from src.ZillowHouseData.utils.common import save_object_to_pickle\n",
    "from src.ZillowHouseData.entity.config_entity import ModelTrainingConfig\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization,Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModeling:\n",
    "        \n",
    "    def __init__(self, config: ModelTrainingConfig):\n",
    "        self.config = config\n",
    "        self.learning_rate = self.config.learning_rate\n",
    "        self.epochs = self.config.epochs\n",
    "        self.batch_size = self.config.batch_size\n",
    "        self.validation_split = self.config.validation_split\n",
    "        self.verbose = self.config.verbose\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        try:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(128, activation='relu', input_shape=(input_shape,)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.3))\n",
    "            model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(0.3))\n",
    "            model.add(Dense(1, kernel_regularizer='l2'))\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def train_model(self, model, X_train, y_train):\n",
    "        try:\n",
    "\n",
    "            params = {\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'epochs': self.epochs,\n",
    "                'batch_size': self.batch_size, \n",
    "                'validation_split': self.validation_split,\n",
    "                'verbose': self.verbose\n",
    "            }\n",
    "\n",
    "            save_object_to_pickle(params, \"models\", \"params.pkl\")\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=self.validation_split, verbose=self.verbose)\n",
    "\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_model_test():  \n",
    "        file_path = os.path.join('artifacts', 'data_ingestion', 'final.csv')\n",
    "        df = read_csv_to_dataframe(file_path)\n",
    "\n",
    "def prepare_data_test(df):        \n",
    "        X, y, label_to_category_mapping = prepare_data(df)\n",
    "        save_object_to_pickle(label_to_category_mapping, \"models\", \"label\")\n",
    "\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test, scaler = train_and_test_split(X, y)\n",
    "\n",
    "        save_object_to_pickle( X_test_scaled, \"models\", \"X_test_scaled.pkl\")\n",
    "        save_object_to_pickle( y_test, \"models\", \"y_test.pkl\")\n",
    "        save_object_to_pickle(scaler, \"models\", \"scaler.pkl\")\n",
    "\n",
    "        config = ConfigurationManager()\n",
    "        model_training_config = config.get_model_training_config()\n",
    "        data_modeling = DataModeling(config = model_training_config)\n",
    "\n",
    "def data_build_test():\n",
    "        # Build model\n",
    "        model = data_modeling.build_model(X_train_scaled.shape[1])\n",
    "\n",
    "def train_model_test():\n",
    "        model = data_modeling.train_model(model, X_train_scaled, y_train)\n",
    "\n",
    "        model_details = model.summary()\n",
    "        logger.info(model_details)\n",
    "\n",
    "def save_model_test():\n",
    "        save_model_to_keras(model, \"models\", \"model.keras\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n",
    "        obj2 = DataModellingPipeline()\n",
    "        obj2.data_model()\n",
    "        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File './artifacts/model/params.pkl' not found.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set the path to the pickle file\n",
    "pickle_file_path = \"./artifacts/model/params.pkl\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(pickle_file_path):\n",
    "    # Load the data from the pickle file\n",
    "    with open(pickle_file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    # Specify the label you want to display\n",
    "    desired_label = \"your_desired_label\"\n",
    "\n",
    "    # Check if the label exists in the data\n",
    "    if desired_label in data:\n",
    "        # Display the content of the desired label\n",
    "        print(f\"Content of '{desired_label}': {data[desired_label]}\")\n",
    "    else:\n",
    "        print(f\"Label '{desired_label}' not found in the pickle file.\")\n",
    "else:\n",
    "    print(f\"File '{pickle_file_path}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of '/Users/keshavkumarelankovan/Desktop/Project_7374_/Zillow-data-ML-project/artifacts/models/label': {7: 'ZATT', 9: 'ZSFH', 6: 'ZALL', 8: 'ZCON', 5: 'ZABT', 4: 'Z5BR', 1: 'Z2BR', 2: 'Z3BR', 0: 'Z1BR', 3: 'Z4BR'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Set the path to the pickle file\n",
    "pickle_file_path = \"/Users/keshavkumarelankovan/Desktop/Project_7374_/Zillow-data-ML-project/artifacts/models/label\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(pickle_file_path):\n",
    "    # Load the data from the pickle file\n",
    "    with open(pickle_file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    # Display the content of the pickle file\n",
    "    print(f\"Content of '{pickle_file_path}': {data}\")\n",
    "else:\n",
    "    print(f\"File '{pickle_file_path}' not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_zillow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
